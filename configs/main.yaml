ROOT_DIR: /root/ryanwang/phdbrainstorm

seed: 0 # DO NOT CHANGE THIS.

exp_name: "olmo2_pubmed-hashprefix_100k_BS-128"
data_type: "pubmed-hashprefix"

num_proc: 1

train:
  do: True
  model_path_or_name: "allenai/OLMo-2-1124-7B"
  out_directory: "${ROOT_DIR}/models"

  wandb:
    do: True
    project: "conditional_learning"
    group: ${data_type}
    name: ${exp_name}
    config:
      data_type: "pubmed-hashprefix"
      batch_size: 128
      num_instances: 100_000
      model: "olmo2-8b"
    tags:
      - "data_type-pubmed-hashprefix"
      - "num_instances-100k"
      - "batch_size-128"
      - "model-olmo2-8b"

  save_model: True

  max_seq_len: 512

  training_args:
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 16 # using 8 gpus rn
    num_train_epochs: 1
    save_steps: 75
#    fp16: True
    bf16: True
    bf16_full_eval: True
    logging_steps: 2
    deepspeed: "configs/ds_config.json"


