ROOT_DIR: /root/ryanwang/phdbrainstorm

seed: 0 # DO NOT CHANGE THIS.

exp_name: "pubmed-hashprefix-20_100k_BS-128"
data_type: "pubmed-hashprefix"

num_proc: 1

train:
  do: True
  model_path_or_name: "meta-llama/Meta-Llama-3-8B"
  out_directory: "${ROOT_DIR}/models"

  wandb:
    do: True
    project: "conditional_learning"
    group: ${data_type}
    name: ${exp_name}
    config:
      data_type: "pubmed-hashprefix-20"
      batch_size: 128
      num_instances: 100_000
      model: "llama3-8b"
      prefix_length: 20
    tags:
      - "data_type-pubmed-hashprefix-20"
      - "num_instances-100k"
      - "batch_size-128"
      - "model-llama3-8b"
      - "prefix_length-20"

  save_model: True

  max_seq_len: 512
  prefix_length: 20 # only valid when data_type is "pubmed-hashprefix"

  training_args:
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 16 # using 8 gpus rn
    num_train_epochs: 1
    save_steps: 75
#    fp16: True
    bf16: True
    bf16_full_eval: True
    logging_steps: 2
    deepspeed: "configs/ds_config.json"


